{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techical Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: (DE-EN combination)\n",
    "      * http://www.statmt.org/europarl/ \n",
    "      * https://tilde-model.s3-eu-west-1.amazonaws.com/Tilde_MODEL_Corpus.html \n",
    "            * Tilde MODEL - ECB\n",
    "            * Tilde MODEL - EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data: \n",
    "    https://github.com/amake/TMX2Corpus\n",
    "\n",
    "    for datasets:              \n",
    "    * Tilde MODEL - ECB\n",
    "    * Tilde MODEL - EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install github repository:\n",
    "pip install git+https://github.com/amake/tmx2corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run files\n",
    "tmx2corpus EMEA/bitext.tok\n",
    "tmx2corpus ECB/bitext.tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Model:\n",
    "\n",
    "https://github.com/tensorflow/nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original project they were used WMT16 German-English data, you can download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmt/scripts/wmt16_en_de.sh /tmp/wmt16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the GNMT WMT German-English model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m nmt.nmt \\\n",
    "    --src=de --tgt=en \\\n",
    "    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\\n",
    "    --out_dir=/tmp/deen_gnmt \\\n",
    "    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\\n",
    "    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \\\n",
    "    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \\\n",
    "    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-trained GNMT WMT German-English checkpoint for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##4-layer EN-DE\n",
    "python -m nmt.nmt --src=en --tgt=de --ckpt=EN-GE_pre-trained_models\\ende_gnmt_model_4_layer\\translate.ckpt --hparams_path=nmt\\standard_hparams\\wmt16_gnmt_4_layer.json --out_dir=tmp\\deen_gnmt --vocab_prefix=tmp\\wmt16\\vocab.bpe.32000 --inference_input_file=tmp\\wmt16\\newstest2014.tok.bpe.32000.en --inference_output_file=tmp\\deen_gnmt\\output_infer --inference_ref_file=tmp\\wmt16\\newstest2014.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: \n",
    "  done, num sentences 3003, num translations per input 1, time 2779s, Mon Nov 20 15:00:51 2017.\n",
    "  bleu: 23.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "{\n",
    "  \"attention\": \"normed_bahdanau\",\n",
    "  \"attention_architecture\": \"standard\",\n",
    "  \"batch_size\": 64,\n",
    "  \"colocate_gradients_with_ops\": true,\n",
    "  \"decay_factor\": 0.5,\n",
    "  \"decay_steps\": 5000,\n",
    "  \"dropout\": 0.2,\n",
    "  \"encoder_type\": \"bi\",\n",
    "  \"eos\": \"&lt;/s&gt;\",\n",
    "  \"forget_bias\": 1.0,\n",
    "  \"infer_batch_size\": 32,\n",
    "  \"init_weight\": 0.1,\n",
    "  \"learning_rate\": 1.0,\n",
    "  \"max_gradient_norm\": 5.0,\n",
    "  \"metrics\": [\"bleu\"],\n",
    "  \"num_buckets\": 5,\n",
    "  \"num_layers\": 4,\n",
    "  \"num_train_steps\": 10000,\n",
    "  \"num_units\": 1024,\n",
    "  \"optimizer\": \"sgd\",\n",
    "  \"residual\": false,\n",
    "  \"share_vocab\": false,\n",
    "  \"subword_option\": \"bpe\",\n",
    "  \"sos\": \"&lt;s&gt;\",\n",
    "  \"source_reverse\": false,\n",
    "  \"src_max_len\": 50,\n",
    "  \"src_max_len_infer\": null,\n",
    "  \"start_decay_step\": 5000,\n",
    "  \"steps_per_external_eval\": null,\n",
    "  \"steps_per_stats\": 100,\n",
    "  \"tgt_max_len\": 50,\n",
    "  \"tgt_max_len_infer\": null,\n",
    "  \"time_major\": true,\n",
    "  \"unit_type\": \"lstm\",\n",
    "  \"beam_width\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ECB data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dev file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mkfifo onerandom tworandom\n",
    "tee onerandom tworandom < /dev/urandom > /dev/null &\n",
    "shuf --random-source=onerandom bitext.en > en.shuf &\n",
    "shuf --random-source=tworandom bitext.de > de.shuf &\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split text to train, test, dev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split shuffled text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize - BPE:\n",
    "    nmt/nmt/scripts/tok-bpe.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmt/scripts/tok-bpe.sh ECB/\n",
    "nmt/scripts/tok-bpe.sh EMEA/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "set -e\n",
    "\n",
    "BASE_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )/..\" && pwd )\"\n",
    "\n",
    "OUTPUT_DIR=\"${1:-wmt16_de_en}\"\n",
    "echo \"Writing to ${OUTPUT_DIR}. To change this, set the OUTPUT_DIR environment variable.\"\n",
    "\n",
    "# Clone Moses\n",
    "if [ ! -d \"${OUTPUT_DIR}/mosesdecoder\" ]; then\n",
    "  echo \"Cloning moses for data processing\"\n",
    "  git clone https://github.com/moses-smt/mosesdecoder.git \"${OUTPUT_DIR}/mosesdecoder\"\n",
    "fi\n",
    "\n",
    "# Tokenize data\n",
    "for f in ${OUTPUT_DIR}/*.de; do\n",
    "  echo \"Tokenizing $f...\"\n",
    "  ${OUTPUT_DIR}/mosesdecoder/scripts/tokenizer/tokenizer.perl -q -l de -threads 8 < $f > ${f%.*}.tok.de\n",
    "done\n",
    "\n",
    "for f in ${OUTPUT_DIR}/*.en; do\n",
    "  echo \"Tokenizing $f...\"\n",
    "  ${OUTPUT_DIR}/mosesdecoder/scripts/tokenizer/tokenizer.perl -q -l en -threads 8 < $f > ${f%.*}.tok.en\n",
    "done\n",
    "\n",
    "# Clean all corpora\n",
    "for f in ${OUTPUT_DIR}/*.en; do\n",
    "  fbase=${f%.*}\n",
    "  echo \"Cleaning ${fbase}...\"\n",
    "  ${OUTPUT_DIR}/mosesdecoder/scripts/training/clean-corpus-n.perl $fbase de en \"${fbase}.clean\" 1 80\n",
    "done\n",
    "\n",
    "# Generate Subword Units (BPE)\n",
    "# Clone Subword NMT\n",
    "if [ ! -d \"${OUTPUT_DIR}/subword-nmt\" ]; then\n",
    "  git clone https://github.com/rsennrich/subword-nmt.git \"${OUTPUT_DIR}/subword-nmt\"\n",
    "fi\n",
    "\n",
    "# Learn Shared BPE\n",
    "for merge_ops in 32000; do\n",
    "  echo \"Learning BPE with merge_ops=${merge_ops}. This may take a while...\"\n",
    "  cat \"${OUTPUT_DIR}/train.tok.clean.de\" \"${OUTPUT_DIR}/train.tok.clean.en\" | \\\n",
    "    ${OUTPUT_DIR}/subword-nmt/learn_bpe.py -s $merge_ops > \"${OUTPUT_DIR}/bpe.${merge_ops}\"\n",
    "\n",
    "  echo \"Apply BPE with merge_ops=${merge_ops} to tokenized files...\"\n",
    "  for lang in en de; do\n",
    "    for f in ${OUTPUT_DIR}/*.tok.${lang} ${OUTPUT_DIR}/*.tok.clean.${lang}; do\n",
    "      outfile=\"${f%.*}.bpe.${merge_ops}.${lang}\"\n",
    "      ${OUTPUT_DIR}/subword-nmt/apply_bpe.py -c \"${OUTPUT_DIR}/bpe.${merge_ops}\" < $f > \"${outfile}\"\n",
    "      echo ${outfile}\n",
    "    done\n",
    "  done\n",
    "\n",
    "  # Create vocabulary file for BPE\n",
    "  echo -e \"<unk>\\n<s>\\n</s>\" > \"${OUTPUT_DIR}/vocab.bpe.${merge_ops}\"\n",
    "  cat \"${OUTPUT_DIR}/train.tok.clean.bpe.${merge_ops}.en\" \"${OUTPUT_DIR}/train.tok.clean.bpe.${merge_ops}.de\" | \\\n",
    "    ${OUTPUT_DIR}/subword-nmt/get_vocab.py | cut -f1 -d ' ' >> \"${OUTPUT_DIR}/vocab.bpe.${merge_ops}\"\n",
    "\n",
    "done\n",
    "\n",
    "# Duplicate vocab file with language suffix\n",
    "cp \"${OUTPUT_DIR}/vocab.bpe.32000\" \"${OUTPUT_DIR}/vocab.bpe.32000.en\"\n",
    "cp \"${OUTPUT_DIR}/vocab.bpe.32000\" \"${OUTPUT_DIR}/vocab.bpe.32000.de\"\n",
    "\n",
    "echo \"All done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters:\n",
    "\n",
    "1 epoch - 26 steps - start decay at 13:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "  \"attention\": \"normed_bahdanau\",\n",
    "  \"attention_architecture\": \"standard\",\n",
    "  \"batch_size\": 128,\n",
    "  \"colocate_gradients_with_ops\": true,\n",
    "  \"decay_factor\": 0.5,\n",
    "  \"decay_steps\": 5000,\n",
    "  \"dropout\": 0.2,\n",
    "  \"encoder_type\": \"bi\",\n",
    "  \"eos\": \"&lt;/s&gt;\",\n",
    "  \"forget_bias\": 1.0,\n",
    "  \"infer_batch_size\": 32,\n",
    "  \"init_weight\": 0.1,\n",
    "  \"learning_rate\": 1.0,\n",
    "  \"max_gradient_norm\": 5.0,\n",
    "  \"metrics\": [\"bleu\"],\n",
    "  \"num_buckets\": 5,\n",
    "  \"num_layers\": 4,\n",
    "  \"num_train_steps\": 10000,\n",
    "  \"num_units\": 1024,\n",
    "  \"optimizer\": \"sgd\",\n",
    "  \"residual\": false,\n",
    "  \"share_vocab\": false,\n",
    "  \"subword_option\": \"bpe\",\n",
    "  \"sos\": \"&lt;s&gt;\",\n",
    "  \"source_reverse\": false,\n",
    "  \"src_max_len\": 50,\n",
    "  \"src_max_len_infer\": null,\n",
    "  \"start_decay_step\": 5000,\n",
    "  \"steps_per_external_eval\": null,\n",
    "  \"steps_per_stats\": 100,\n",
    "  \"tgt_max_len\": 50,\n",
    "  \"tgt_max_len_infer\": null,\n",
    "  \"time_major\": true,\n",
    "  \"unit_type\": \"lstm\",\n",
    "  \"beam_width\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained model infered on ecb test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m nmt.nmt \\\n",
    "    --src=en --tgt=de \\\n",
    "    --ckpt=../ende_gnmt_model_8_layer/translate.ckpt \\\n",
    "    --hparams_path=nmt/standard_hparams/wmt16_gnmt_8_layer.json \\\n",
    "    --out_dir=../inference2/ende_mymodel \\\n",
    "    --vocab_prefix=wmt16_de_en/vocab.bpe.32000 \\\n",
    "    --inference_input_file=../ecb/test.tok.bpe.32000.en \\\n",
    "    --inference_output_file=../inference2/ende_mymodel/output_infer \\\n",
    "    --inference_ref_file=../ecb/test.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bleu score: 22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own model trained on economics dataset infered on the ecb test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m nmt.nmt \\\n",
    "    --src=en --tgt=de \\\n",
    "    --ckpt=../25nov-training/translate.ckpt-32000 \\\n",
    "    --hparams_path=nmt/standard_hparams/myconfig.json \\\n",
    "    --out_dir=../inference/ende_mymodel \\\n",
    "    --vocab_prefix=../ecb/vocab.bpe.32000 \\\n",
    "    --inference_input_file=../ecb/test.tok.bpe.32000.en \\\n",
    "    --inference_output_file=../inference/ende_mymodel/output_infer \\\n",
    "    --inference_ref_file=../ecb/test.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bleu score: 19.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained model infered on Medicinal test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m nmt.nmt \\\n",
    "    --src=en --tgt=de \\\n",
    "    --ckpt=../ende_gnmt_model_8_layer/translate.ckpt \\\n",
    "    --hparams_path=nmt/standard_hparams/wmt16_gnmt_8_layer.json \\\n",
    "    --out_dir=../inference3/ \\\n",
    "    --vocab_prefix=wmt16_de_en/vocab.bpe.32000 \\\n",
    "    --inference_input_file=../EMEA/test.tok.bpe.32000.en \\\n",
    "    --inference_output_file=../inference3/output_infer \\\n",
    "    --inference_ref_file=../EMEA/test.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue score: 17.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own model trained on medicinal data set infered on Medicinal test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m nmt.nmt \\\n",
    "    --src=en --tgt=de \\\n",
    "    --ckpt=../emea-model/translate.ckpt-16000 \\\n",
    "    --hparams_path=nmt/standard_hparams/myconfig.json \\\n",
    "    --out_dir=../inference4/ \\\n",
    "    --vocab_prefix=../EMEA/vocab.bpe.32000 \\\n",
    "    --inference_input_file=../EMEA/test.tok.bpe.32000.en \\\n",
    "    --inference_output_file=../inference4/output_infer \\\n",
    "    --inference_ref_file=../EMEA/test.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue score: 30.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensooard --port 22222 --logdir ECB/deen_gnmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
